ðŸ§  FastAPI + GPT4All Hybrid Techbot

A GPU-accelerated technical assistant designed for Tier 3 network - database - software support, built with FastAPI, FAISS, and GPT4All.


ðŸš€ Features
Local LLM (Meta-Llama-3-8B via GPT4All)

FAISS-powered semantic RAG (retrieval-augmented generation)

Fully offline â€” no OpenAI API required

Custom prompt persona

Frontend served directly via FastAPI

ðŸ—‚ Project Structure

â”œâ”€â”€ app/

â”‚   â”œâ”€â”€ main.py            # FastAPI backend

â”‚   â”œâ”€â”€ qa_logic.py        # GPT4All integration

â”‚   â”œâ”€â”€ data_loader.py     # Loads FAISS index + Q&A pairs

â”‚   â””â”€â”€ __init__.py

â”œâ”€â”€ static/

â”‚   â”œâ”€â”€ frontend.html      # Chat UI

â”‚   â””â”€â”€ your-logo.png       # Background image

â”œâ”€â”€ data/

â”‚   â””â”€â”€ DMQAPairs.csv      # Source data for FAISS

â”œâ”€â”€ requirements.txt

â””â”€â”€ README.md

ðŸ›  Setup
bash

# Clone repo
git clone https://github.com/youruser/my-qa-bot.git
cd my-qa-bot

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

ðŸ”— Dependencies
Install system packages:

bash
sudo apt update && sudo apt install build-essential libopenblas-dev

Install Python packages:

pip install fastapi uvicorn numpy faiss-cpu sentence-transformers gpt4all

ðŸ’¾ Model Setup

Download the model:

From TheBloke/Llama-3-8B-Instruct-GGUF

Place .gguf file in:
~/.cache/gpt4all/

Set this path in qa_logic.py:

MODEL_PATH = os.path.expanduser("~/.cache/gpt4all/<your_model>.gguf")

ðŸ§  Run It

source venv/bin/activate
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

Then visit:

http://localhost:8000/

ðŸ“¦ Ignore These in Git

venv/
*.gguf
__pycache__/
.cache/
.gpt4all/
